{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KGzB/CAS-Applied-Data-Science/blob/master/Module-1/M1-D1-DM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOSMpeOK06p8"
      },
      "source": [
        "Notebook 1, Module 1, Data and Data Management, CAS Applied Data Science, 2024-08-21, A. Mühlemann, University of Bern. (based on the template by S. Haug)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljJDZt-n6zPw"
      },
      "source": [
        "Prerequisite for this notebook is some basic Python experience.\n",
        "\n",
        "Please also look at the first batch of [the slides](https://drive.google.com/file/d/12cWTQG5_vcoaz-puhBgH9GrfKUADmLdo/view?usp=sharing) before doing this notebook. They offer an introduction to data.\n",
        "\n",
        "ChatGPT is able to write most of the code needed for this notebook. Please use it if you like!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAVKazfU06p-"
      },
      "source": [
        "# 1. Data Management\n",
        "\n",
        "Estimated study time is about 2 hours. According to your background and how much you want to learn, you may need more or less. You are supposed to google, read manuals and chat with others during working through this notebook in order to benefit fully.\n",
        "\n",
        "**Learning outcomes - after completion you**\n",
        "- Know about data sources, types and formats (see lecture slides via link above)\n",
        "- Able to import and export data in Python\n",
        "- Able to do simple things with dataframes in Python\n",
        "\n",
        "**Documentation on Pandas DataFrame**\n",
        "- Python: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
        "\n",
        "## Outline\n",
        "\n",
        "     0. About data management\n",
        "     1. Getting used to Jupyter notebooks / colab\n",
        "     2. Importing and merging\n",
        "     3. Indexing on a dataframe\n",
        "     4. Sorting\n",
        "     5. Filtering\n",
        "     6. Exporting\n",
        "     7. Missing and bad data\n",
        "     8. Metadata\n",
        "     9. Working on the filesystem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xY2gyiE06qA"
      },
      "source": [
        "## 0. About data management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFyhF-jp06qB"
      },
      "source": [
        "Handling or managing data involves many steps and technologies at many levels. Data may be colleccted by sensors. It can be a camera, a temperature sensor, a telescope, a microscope (with a camera), a microphone, a particle detector etc. Normally the data is then digitised, maybe preprocessed and written to some media in a certain format, e.g. as a comma separated value (csv) file to a hard disk. This part of the data management is normally taken care of by engineers.\n",
        "\n",
        "Data may also be collected from all sorts of databases, so data already collected somehow. Time series of financial data, customers, passengers, facebook likes, twitter tweets etc. This is data which is normally already on a media with some interface for access, e.g. paper to be read by a camera, a file on youtube, a table on wikipedia etc. We will look at some ways to collect such data. Some programming and computer skills are needed to do so. It may be that this part of the data management is taken care of by specialised computer scientists, but it may also be expected from a data scientist to have these skills.\n",
        "\n",
        "Analysing data with statisitical and machine learning tools, requires that the data is colleceted, cleaned and prepared for the tools. *This is very often a very large part of a data analytics project and a prerequisite*. It may involve removing bad data, filter out redundant and noisy data, unify the formats and types, transform the data etc.\n",
        "Thus, a data scientist must be able to perform this part of the data management. This notebook shows the basic operations with Python pandas. With other tools the concepts and operations are very similar.\n",
        "\n",
        "After the data analysis, after the extraction of information and the creation of knowledge, the data is often stored or archived for the future (if this seems cheaper than regenerating the data at a later point). In larger institutions this part of the data management may include educated librarians and others, not necessarily the data scientist.  \n",
        "\n",
        "**In this notebook we only look at a few examples on how to do datamanagement with dataframes. Pandas are extremly powerful and we cannot show everything in a couple of hours. You will become more and more experienced when you work on your module projects. Probably whatever you want to do with your dataframe, there is a way to do it. If not, it probably doesn't make much sense what wou want to do.**\n",
        "\n",
        "Any questions?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4HJ8cnp06qC"
      },
      "source": [
        "## 1. Getting used to Jupyter computational notebooks and Colab\n",
        "\n",
        "With Jupyter you can write rich text notebooks with executable code via your browser. There are several so-called kernels or computational back ends, i.e Python, R, Julia, bash etc can be supported. The text is written as Markdown. Latex is also supported (good for math). You can export the notebook in various formats, e.g. html. Everything can be done via the various tabs, however, the key shortcuts make you faster.\n",
        "\n",
        "*Useful key combinations*\n",
        "\n",
        "- Shift Enter or Control Enter = Run cell\n",
        "- Option/Alt Enter = Run cell and Insert new cell below\n",
        "\n",
        "### Exercise 1 (10 min)\n",
        "- Change and run this cell\n",
        "- Add a new cell and execute some python statement in it\n",
        "- Study the tabs in the menu of your jupyter (lab) notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tn_Bnh606qC",
        "outputId": "172b7878-939a-4f4d-dc6d-ec0c70a3daf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello world!\n"
          ]
        }
      ],
      "source": [
        "# Write some more Python code here\n",
        "print(\"hello world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yy0P4Q006qD"
      },
      "source": [
        "## 2. Import a dataset into a Pandas DataFrame\n",
        "\n",
        "Pandas is a Python Module for data analysis and manipulation and management. It has the data structure DataFrame which is quite powerful. Features\n",
        "\n",
        "- DataFrame object for data management with integrated indexing.\n",
        "- Tools for reading and writing data between in-memory data structures and different file formats.\n",
        "- Data alignment and integrated handling of missing data.\n",
        "- Reshaping and pivoting of data sets.\n",
        "- Label-based slicing, fancy indexing, and subsetting of large data sets.\n",
        "- Data structure column insertion and deletion.\n",
        "- Group by engine allowing split-apply-combine operations on data sets.\n",
        "- Data set merging and joining.\n",
        "- Hierarchical axis indexing to work with high-dimensional data in a lower-dimensional data structure.\n",
        "- Time series-functionality: Date range generation and frequency conversion, moving window statistics, moving window linear regressions, date shifting and lagging.\n",
        "\n",
        "The module is highly optimized for performance, with critical code parts written in Cython or C. Documentation here: https://pandas.pydata.org/pandas-docs/stable/api.html\n",
        "\n",
        "For input/output (I/O) there are methods for reading EXCEL, SQL databases, HTML tables, clipboard, SAS, STATA etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlOvr5s1uvbu"
      },
      "source": [
        "In this notebook we will work with a dataset that contains brain and body weight, life  span, gestation time,  sleep and danger indices of 62 mammals [(source)](https://lib.stat.cmu.edu/datasets/sleep).\n",
        "\n",
        "To better illustrate some challenges faced when working with data, I split the data set into *sleep.csv* and *danger.csv* that can be downloaded from Github.\n",
        "\n",
        "The data set *sleep.csv* contains:\n",
        "* Species: species of animal\n",
        "* BodyWt: body weight in kg\n",
        "* BrainWt: brain weight in g\n",
        "* NonDreaming: slow wave (\"nondreaming\") sleep (hrs/day)\n",
        "* Dreaming: paradoxical (\"dreaming\") sleep (hrs/day)\n",
        "* TotalSleep: total sleep (hrs/day)  (sum of slow wave and paradoxical sleep)\n",
        "* LifeSpan: maximum life span (years)\n",
        "* Gestation: gestation time (days)\n",
        "\n",
        "The data set *danger.csv* contains:\n",
        "* Species: species of animal\n",
        "* Predation: predation index (1-5)\\\n",
        "1 = minimum (least likely to be preyed upon)\\\n",
        "5 = maximum (most likely to be preyed upon)\n",
        "* Exposure: sleep exposure index (1-5)\\\n",
        "1 = least exposed (e.g. animal sleeps in a well-protected den)\\\n",
        "5 = most exposed\n",
        "* Danger: overall danger index (1-5) based on the above two indices and other information\n",
        "1 = least danger (from other animals)\\\n",
        "5 = most danger (from other animals)\n",
        "\n",
        "\n",
        "### Exercise 2\n",
        "- Upload both files to Colab\n",
        "- Import the python module pandas with abbriveation pd\n",
        "- Read both data sets into a pd dataframe\n",
        "- Look at both dataframes and identify possible problems that could surface when trying to merge the dataframes\n",
        "\n",
        "#### Hint\n",
        "\n",
        "There are some ways to get help about modules and methods directly in Jupyter:\n",
        "\n",
        "```\n",
        "dataframe. #(hover over the object to see its methods)\n",
        "```\n",
        "or\n",
        "\n",
        "```\n",
        "dataframe?\n",
        "```\n",
        "\n",
        "And, what you will probably use the most, the online package information and examples by googling or ChatGPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "2va7jftt06qF",
        "outputId": "5872a374-ae37-4cbc-9704-6081a390c3a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fc456cec-2154-41ae-85ef-f0aa664dde39\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fc456cec-2154-41ae-85ef-f0aa664dde39\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Upload the local data file to colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcu5tlR106qG"
      },
      "outputs": [],
      "source": [
        "# import the python module pandas with the abbreviation pd\n",
        "\n",
        "# Read the data in the sleep.csv file into a dataframe\n",
        "\n",
        "# Read the data in the danger.csv file into a dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEOk5TwFzusC"
      },
      "outputs": [],
      "source": [
        "# show sleep.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKvS45Aq06qH"
      },
      "source": [
        "The dataframe method shows 50 rows per default. We can change with\n",
        "\n",
        "```\n",
        "pd.set_option('display.max_rows', None)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odf02zpP06qI"
      },
      "outputs": [],
      "source": [
        "# show danger.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKQOX0Id0Vn5"
      },
      "source": [
        "Some of the callenges are that in *sleep.csv* each row corresponds to a species, while for *danger.csv* it's the columns that correspond to a species. Moreover, in *sleep.csv* the species are sorted from A-Z, while for *danger.csv* they are sorted from Z-A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNySrhxP06qK"
      },
      "source": [
        "### Exercise 3\n",
        "Merge the two dataframes. We take *sleep.csv* as are base and want to add the information of *danger.csv* to the sleep dataframe. There are of course multiple solutions but try to make de modifications in Python and not with Excel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPP9qMe313vQ"
      },
      "outputs": [],
      "source": [
        "# transpose danger.csv\n",
        "\n",
        "# merge danger with sleep\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t12tFHM_612N"
      },
      "source": [
        "### Exercise 4\n",
        "Investigate what ```how='inner'``` does and what other options exist.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUECfDlm7MIo"
      },
      "source": [
        "The expression ```how='inner'``` performs an inner join between the two dataframes meaning that only species existing in both dataframes end up in the merged dataframe.\n",
        "\n",
        "Other options are:\n",
        "- ```how='left'```: includes all rows from the left dataframe, and matching rows from the right dataframe.\n",
        "\n",
        "- ```how='right'```: includes all rows from the right dataframe, and matching rows from the left dataframe.\n",
        "\n",
        "- ```how='outer'```: includes all rows from both data frames, with NaNs where there are no matches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCI7ub-706qN"
      },
      "source": [
        "## 3. Indexing on a dataframe\n",
        "From now on we work with our newly merged dataframe.\n",
        "Indexing is quite useful to\n",
        "- directly access a specific element in a dataframe.\n",
        "- easily modify or assign values.\n",
        "- filter.\n",
        "- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHJVki3j-4q_"
      },
      "source": [
        "We can access a single cell by indexing my its row and column\n",
        "\n",
        "```\n",
        "dataframe.iat[row_index,column_index]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYaVBJC9_Z-4"
      },
      "source": [
        "### Exercise 5\n",
        "Check what value lies in colum 4 row 58"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0sDmvr006qN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXFjEF1LAngT"
      },
      "source": [
        "We can also assign a new value to this cell using\n",
        "\n",
        "```\n",
        "dataframe.iat[row_index,column_index]=10\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfkKtXeWA-Fq"
      },
      "source": [
        "### Exercise 6\n",
        "Overwrite the value in row 58 and column 4 with 10 and then change iz back to 4.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG7Kfw-3AXy7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGbwktSNBVkV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttVndN1x_tgj"
      },
      "source": [
        "We can also access multiple cells by\n",
        "\n",
        "\n",
        "```\n",
        "dataframe.iloc[min_row:max_row+1,min_col:max_col+1]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868_drwdB0LY"
      },
      "source": [
        "### Exercise 7\n",
        "Look at the first 10 row and the last 3 columns of our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeGXg-IOB8Te"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf6gF3jr06qN"
      },
      "source": [
        "**Important**\n",
        "When you assign a (subset) of dataframe to a new one like above, no copy is made. This means that if you change values of the new frame, also the orginal frame will be changed. If you want a copy, you need to use the copy method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n8pa6UH06qO"
      },
      "source": [
        "We can also access colums by the column names.\n",
        "### Exercise 8\n",
        "Look at the *TotalSleep* of all species. and in a second step look at *TotalSleep* and *BrainWt* of the first sevel rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZapUeo_x06qO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Sqgdk106qP"
      },
      "source": [
        "Selecting by row values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPqUoUYR06qP"
      },
      "outputs": [],
      "source": [
        "dataframe[dataframe['Species']=='ArcticFox']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peiXnTgAxw08"
      },
      "source": [
        "Sometimes it is also helpful to group. For example, if we are interested in the average *BrainWt* with respect to the danger the mammals are in, then grouping by *Danger* would yields the result:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK5KGhl4EsVF"
      },
      "outputs": [],
      "source": [
        "dataframe.groupby('Danger')['BrainWt'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHEAmJM5FmGj"
      },
      "source": [
        "### Exercise 9\n",
        "Look at the mean gestation grouped by sleep exposure. Is there some visible difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1fRkA2nF7se"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkS6YD7V06qQ"
      },
      "source": [
        "## 4. Sorting\n",
        "Sorting is helpful because it allows you to organize data in a meaningful order, making it easier to identify patterns, trends, or outliers.\n",
        "\n",
        "We can sort with respect to one or multiple columns as follows.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "dataframe.sort_values(['col1','col2'])\n",
        "```\n",
        "\n",
        "### Exercise 9\n",
        "Sort the dataframe with respect to time the mammals spend sleeping in total as well as dreaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghnoA6O306qQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6I2Xd1d06qQ"
      },
      "source": [
        "## 5. Filtering\n",
        "Filtering dataframes is helpful because it allows you to focus on specific subsets of data that meet certain criteria, simplifying analysis and making it easier to identify trends or anomalies. It also improves efficiency by reducing the volume of data processed, which speeds up computations and insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnHkNsPjIxPu"
      },
      "source": [
        "### Exercise 10\n",
        "- Filter the data set such that only mammals remain that have a lifespan of more than 15 years.\n",
        "- Filter the data set such that only mammals remain that have a lifespan of more than 15 years and have a danger index of 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYCwukeI06qR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpvpPn9BJLPH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg_y8Ift06qR"
      },
      "source": [
        "## 6. Missing or bad data\n",
        "\n",
        "Datasets, before they are \"cleaned\", may contain missing or wrongly formated values. There are DataFrame methods to deal with this:\n",
        "\n",
        "- DataFrame.dropna([axis, how, thresh, …])\tRemove missing values.\n",
        "- DataFrame.fillna([value, method, axis, …])\tFill NA/NaN values using the specified method\n",
        "- DataFrame.replace([to_replace, value, …])\tReplace values given in to_replace with value.\n",
        "- DataFrame.interpolate([method, axis, limit, …])\tInterpolate values according to different methods.\n",
        "\n",
        "Retrieving and cleaning data is often the most time consuming part in a data science project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN7OXDjJ06qR"
      },
      "source": [
        "## 7. Exporting dataframes (I/O)\n",
        "\n",
        "DataFrame has several export methods. (html, hdf5, ascii, excel etc). Let's save file in csv format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhxN8bF_06qR"
      },
      "outputs": [],
      "source": [
        "dataframe.to_csv('merged.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Y9fLm6YYlH"
      },
      "source": [
        "Can you find it?\n",
        "\n",
        "What happens with our files when we close our colab session?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1No9Vhr06qS"
      },
      "source": [
        "## 8. Metadata\n",
        "\n",
        "\n",
        "Metadata is data about the data, e.g. when was it collected, under which conditions, calibration etc.\n",
        "Metadata is normally not part of the statistical data analysis, however, needed for understanding and reproducibilty.\n",
        "\n",
        "DataFrame is not really made for storing metadata (should be done separately), but one can add new attributes to a dataframe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ7Q2lT306qS",
        "outputId": "7fb80574-4663-4a5e-a2f2-6c8354c55a80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Location': 'Gaul', 'Authors': ['Asterix', 'Obelix', 'Idefix']}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata = {\n",
        "    'Location': \"Gaul\",\n",
        "    'Authors': ['Asterix', \"Obelix\", 'Idefix']\n",
        "}\n",
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml-Nha8Dc5hv"
      },
      "source": [
        "Data repositories often use so-called Data Cards for metadata. See for example: https://www.kaggle.com/datasets/uciml/iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5shqbnTGLqBa"
      },
      "source": [
        "## 9. Two more Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCEPcsaf06qS"
      },
      "source": [
        "### Exercise 11\n",
        "Download the iris.csv from Github. Read in the iris dataset into a dataframe. Set the values in column 1 in row 39, 49 and 100 to NaN (use the nan method from the numpy package). Then replacethe NaN values to the average value of the respective column. Depending on how you do it, this may be about 10 lines of Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0GykCj7Ovcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 12\n",
        "This exercise presents a real-world scenario based on data collected from a student survey for a university course on statistics for economics and social sciences. To practice data handling in Python, you'll work with this dataset to prepare it for analysis.\n",
        "\n",
        "The survey gathered responses on five variables:\n",
        "\n",
        "* Mobile OS: The operating system of the student's mobile phone.\n",
        "* Screen time: Daily screen time in minutes.\n",
        "* Shopping preference: The student's preferred method of shopping (online or in-store).\n",
        "* Favorite subject: Their favorite subject during high school.\n",
        "* Weekly sports time: The number of hours spent on sports each week.\n",
        "\n",
        "The raw datasets are available for download on ILIAS. Your task is to merge and clean the data to make it usable. This includes: Identify and handle any data points that are clearly incorrect or nonsensical. The \"favorite subject\" variable is particularly messy due to free-form text input. You'll need to\n",
        "develop a strategy to categorize these answers consistently. There isn't a single correct approach, but this task demonstrates how crucial careful study design is to avoid common data cleaning issues."
      ],
      "metadata": {
        "id": "QE1iTwuqzxlu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdq_oJYR4cf5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}